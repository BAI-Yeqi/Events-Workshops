{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "min_length = 2\n",
    "max_length = 20\n",
    "target_folder_name = 'min_2_max_20'\n",
    "project_folder = '/data/workspace/yeqi/projects/RNN4REC/GRU4REC'\n",
    "data_folder = '/data/workspace/yeqi/projects/RNN4REC/GRU4REC/Data/paths'\n",
    "\n",
    "train_folder = data_folder + '/training set'\n",
    "test_folder = data_folder + '/test set'\n",
    "full_folder = data_folder + '/full data'\n",
    "\n",
    "pro_data_folder = project_folder + '/Processed Data'\n",
    "target_folder = pro_data_folder + '/' + target_folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(target_folder):\n",
    "    os.makedirs(target_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for processing all the path files in folder\n",
    "def txt2list(txt_path):\n",
    "    '''\n",
    "    input:\n",
    "        1. txt_path: path to the text file\n",
    "    output:\n",
    "        2. a list containing individual lists, each of which contains:\n",
    "            userid, pathid, itemids in path\n",
    "    '''\n",
    "    results = []\n",
    "    with open(txt_path) as inputfile:\n",
    "        for line in inputfile:\n",
    "            results.append(line.strip().split('|'))\n",
    "            \n",
    "    valid_results = []\n",
    "    for row in results:\n",
    "        if ',' in row[2]:\n",
    "            valid_results.append(row)\n",
    "            \n",
    "    return valid_results\n",
    "\n",
    "def get_paths_from_folder(folder_path):    \n",
    "    return [join(folder_path, f) for f in listdir(folder_path) if isfile(join(folder_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder2arr(folder_path):\n",
    "    # this is a list of all the daily paths (txtfiles)\n",
    "    file_paths = get_paths_from_folder(folder_path)\n",
    "\n",
    "    all_paths = []\n",
    "    for txt_path in file_paths:\n",
    "        if not txt_path.endswith('.DS_Store'):\n",
    "            all_paths = all_paths + txt2list(txt_path)\n",
    "    len(all_paths)    \n",
    "\n",
    "    arr = np.array(all_paths)\n",
    "    arr = arr.astype(object)\n",
    "\n",
    "    # process array so that in each row: element 0 = userid, element 1 = sessid, element 2 = list of itemids\n",
    "    for j in range(len(arr)):\n",
    "        arr[j][0] = int(arr[j][0])\n",
    "        arr[j][1] = int(arr[j][1])\n",
    "        arr[j][2] = list(arr[j][2].split(','))\n",
    "        for i in range(len(arr[j][2])):\n",
    "            arr[j][2][i] = int(arr[j][2][i])\n",
    "\n",
    "    # create a list containing the session lengths\n",
    "    sess_len_list = []\n",
    "    for i in range(arr.shape[0]):\n",
    "        sess_len_list.append(len(arr[i][2]))\n",
    "    sess_len_arr = np.array(sess_len_list)\n",
    "    sess_len_arr = sess_len_arr.reshape([len(sess_len_list),1])\n",
    "\n",
    "    # concatenate the array so that in each row element 3 = session length (number of items presented)\n",
    "    arr = np.concatenate((arr,sess_len_arr),axis = 1)\n",
    "    \n",
    "    # create a new array with desired session length\n",
    "    new_arr = []\n",
    "    for row in arr:\n",
    "        if row[3] >= min_length and row[3] <= max_length:\n",
    "            new_arr.append(row)\n",
    "    new_arr = np.array(new_arr)\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = folder2arr(full_folder)\n",
    "train_arr = folder2arr(train_folder)\n",
    "test_arr = folder2arr(test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data is as followed: \n",
      "    sess_length    count  start_index  end_index\n",
      "0             2  1592038            0    1592038\n",
      "1             3   392060      1592038    1984098\n",
      "2             4   153887      1984098    2137985\n",
      "3             5    78750      2137985    2216735\n",
      "4             6    44196      2216735    2260931\n",
      "5             7    27154      2260931    2288085\n",
      "6             8    18229      2288085    2306314\n",
      "7             9    13102      2306314    2319416\n",
      "8            10     8727      2319416    2328143\n",
      "9            11     7256      2328143    2335399\n",
      "10           12     5661      2335399    2341060\n",
      "11           13     4090      2341060    2345150\n",
      "12           14     3227      2345150    2348377\n",
      "13           15     2860      2348377    2351237\n",
      "14           16     2226      2351237    2353463\n",
      "15           17     1559      2353463    2355022\n",
      "16           18     1373      2355022    2356395\n",
      "17           19     1240      2356395    2357635\n",
      "18           20      797      2357635    2358432\n"
     ]
    }
   ],
   "source": [
    "# np array to dataframe for better statistics\n",
    "df = pd.DataFrame(arr)\n",
    "df.columns = ['userid', 'sess_id', 'sess_path', 'sess_length']\n",
    "\n",
    "df = df.sort_values(by = 'sess_length')\n",
    "df = df.reset_index(drop = 'True')\n",
    "\n",
    "summary = df.groupby('sess_length').count()\n",
    "summary = pd.DataFrame(summary.drop(columns=['sess_id','sess_path']))\n",
    "summary = summary.reset_index()\n",
    "summary.columns = ['sess_length', 'count']\n",
    "\n",
    "slice_table = summary\n",
    "slice_table['start_index'] = slice_table['count']\n",
    "slice_table['end_index'] = slice_table['count']\n",
    "\n",
    "# here is the logic for create the start index + end index\n",
    "for i in range(1, len(slice_table)):\n",
    "    slice_table['end_index'][i] = slice_table['end_index'][i-1] + slice_table['count'][i]\n",
    "\n",
    "for i in range(1, len(slice_table)):\n",
    "    slice_table['start_index'][i] = slice_table['end_index'][i-1]\n",
    "\n",
    "slice_table['start_index'][0] = 0\n",
    "\n",
    "print('The raw data is as followed: ')\n",
    "print(slice_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start processing at Mon Sep 24 09:42:59 2018\n"
     ]
    }
   ],
   "source": [
    "arr = df.values\n",
    "\n",
    "print(\"start processing at\", time.ctime())\n",
    "\n",
    "# now we gonna count the active users and active items\n",
    "userid_list = []\n",
    "itemid_list = []\n",
    "for i in range(arr.shape[0]):\n",
    "    userid_list.append(arr[i][0])\n",
    "    itemid_list = itemid_list + arr[i][2]\n",
    "    \n",
    "userid_set = set(userid_list)\n",
    "itemid_set = set(itemid_list)\n",
    "\n",
    "print('Total amount of user presented before and after dropping duplication:')\n",
    "print(len(userid_list), len(userid_set))\n",
    "print('Total amount of item presented before and after dropping duplication:')\n",
    "print(len(itemid_list), len(itemid_set))\n",
    "\n",
    "print(\"end processing at\", time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid_df = pd.DataFrame([0]+list(userid_set))\n",
    "itemid_df = pd.DataFrame([0]+list(itemid_set))\n",
    "userid_df.columns = ['userid']\n",
    "itemid_df.columns = ['itemid']\n",
    "userid_df['userid'] = pd.to_numeric(userid_df['userid'])\n",
    "itemid_df['itemid'] = pd.to_numeric(itemid_df['itemid'])\n",
    "\n",
    "userid_df = userid_df.sort_values(by = 'userid').reset_index(drop=True)\n",
    "itemid_df = itemid_df.sort_values(by = 'itemid').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid_df.to_csv(target_folder + '/userid_map.csv')\n",
    "itemid_df.to_csv(target_folder + '/itemid_map.csv')\n",
    "\n",
    "# below is the dictionary for mapping item/user ids in shopee db into model based id\n",
    "userid_dict = dict([(userid_df['userid'][i], i) for i in range(len(userid_df))])\n",
    "itemid_dict = dict([(itemid_df['itemid'][i], i) for i in range(len(itemid_df))])\n",
    "\n",
    "def sort_and_map(arr):\n",
    "    # sorting\n",
    "    df = pd.DataFrame(arr)\n",
    "    df.columns = ['userid', 'sess_id', 'sess_path', 'sess_length']\n",
    "    df = df.sort_values(by = 'sess_length')\n",
    "    df = df.reset_index(drop = 'True')\n",
    "    arr = df.values\n",
    "    # mapping the shopee userid/itemid into training data index\n",
    "    for i in range(arr.shape[0]):\n",
    "        arr[i][0] = userid_dict[arr[i][0]]\n",
    "        for j in range(len(arr[i][2])):\n",
    "            arr[i][2][j] = itemid_dict[arr[i][2][j]]\n",
    "    return arr\n",
    "\n",
    "test_arr = sort_and_map(test_arr)\n",
    "train_arr = sort_and_map(train_arr)\n",
    "\n",
    "def add_label(arr):\n",
    "    Y_list = []\n",
    "    for i in range(arr.shape[0]):\n",
    "        Y_list.append(arr[i][2][-1])\n",
    "        arr[i][2] = arr[i][2][:-1]\n",
    "    Y_arr = np.array(Y_list)\n",
    "    Y_arr = Y_arr.reshape([len(Y_list),1])\n",
    "\n",
    "    arr = np.concatenate((arr,Y_arr),axis = 1)\n",
    "    return arr\n",
    "\n",
    "test_arr = add_label(test_arr)\n",
    "train_arr = add_label(train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets_list = [5,6,10,20,50,100]\n",
    "# we deduct all elements in the buckets_list by 1, to fulfill the length of path without Y item\n",
    "for i in range(len(buckets_list)):\n",
    "    buckets_list[i] = buckets_list[i] - 1\n",
    "    \n",
    "def pad2buckets(buckets_list, in_arr):\n",
    "    '''\n",
    "    Input:\n",
    "        1. buckets_list: a list of ints, indicating the step lengths of data we want to generate\n",
    "        2. data_arr: the half-processed data array\n",
    "    Output: \n",
    "        1. pro_arr: the processed array\n",
    "    '''\n",
    "    data_arr = in_arr\n",
    "    # cursor labels the current max length of our training data\n",
    "    cursor = 0\n",
    "    for i in range(data_arr.shape[0]):\n",
    "        # the fourth (index 3) column is the session length of the current user path\n",
    "        if len(data_arr[i][2]) > buckets_list[-1]:\n",
    "            break\n",
    "        # move the cursor to the correct place, \n",
    "        # by right the max_length should be bigger or equal to the length of the current row of data\n",
    "        while len(data_arr[i][2]) > buckets_list[cursor]:\n",
    "            cursor = cursor + 1\n",
    "        \n",
    "        if len(data_arr[i][2]) == buckets_list[cursor]:\n",
    "            data_arr[i][2] = np.array(data_arr[i][2])\n",
    "        if len(data_arr[i][2]) < buckets_list[cursor]:\n",
    "            # create a temp numpy array \n",
    "            #temp_path = np.array([0 for i in range(buckets_list[cursor])])\n",
    "            for j in range(buckets_list[cursor]-len(data_arr[i][2])):\n",
    "                # temp_path[j] = data_arr[i][2][j]\n",
    "                data_arr[i][2].append(0)\n",
    "            # data_arr[i][2] = temp_path\n",
    "            data_arr[i][2] = np.array(data_arr[i][2])\n",
    "            data_arr[i][3] = buckets_list[cursor] + 1\n",
    "    print(i)       \n",
    "    # now i is at the first row we want to abandon\n",
    "    return data_arr[:i]\n",
    "\n",
    "test_arr = pad2buckets(buckets_list, test_arr)\n",
    "train_arr = pad2buckets(buckets_list, train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(pro_arr):\n",
    "    pro_df = pd.DataFrame(pro_arr)\n",
    "    pro_df.columns = ['userid', 'pathid', 'path', 'sess_length', 'Y']\n",
    "    # derive the summary/slice table for the training data\n",
    "    summary = pro_df.groupby('sess_length').count()\n",
    "    summary = pd.DataFrame(summary.drop(columns=['pathid','path','Y']))\n",
    "    summary = summary.reset_index()\n",
    "    summary.columns = ['sess_length', 'count']\n",
    "\n",
    "    slice_table = summary\n",
    "    slice_table['start_index'] = slice_table['count']\n",
    "    slice_table['end_index'] = slice_table['count']\n",
    "    # here is the logic for create the start index + end index\n",
    "    for i in range(1, len(slice_table)):\n",
    "        slice_table['end_index'][i] = slice_table['end_index'][i-1] + slice_table['count'][i]\n",
    "\n",
    "    for i in range(1, len(slice_table)):\n",
    "        slice_table['start_index'][i] = slice_table['end_index'][i-1]\n",
    "\n",
    "    slice_table['start_index'][0] = 0\n",
    "    return slice_table\n",
    "\n",
    "print(\"Here is the summarization of the training data: \")\n",
    "print(summarize(train_arr))\n",
    "print(\"Here is the summarization of the test data: \")\n",
    "print(summarize(test_arr))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(target_folder +'/X_train.npy', train_arr)\n",
    "np.save(target_folder +'/X_test.npy', test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
