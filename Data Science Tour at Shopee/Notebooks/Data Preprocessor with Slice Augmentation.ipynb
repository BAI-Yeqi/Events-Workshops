{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "min_length = 5\n",
    "max_length = 20\n",
    "target_folder_name = 'min_5_max_20'\n",
    "project_folder = '/data/workspace/yeqi/projects/RNN4REC/GRU4REC'\n",
    "data_folder = '/data/workspace/yeqi/projects/RNN4REC/GRU4REC/Data/paths'\n",
    "\n",
    "train_folder = data_folder + '/training set'\n",
    "test_folder = data_folder + '/test set'\n",
    "full_folder = data_folder + '/full data'\n",
    "\n",
    "pro_data_folder = project_folder + '/Processed Data'\n",
    "target_folder = pro_data_folder + '/' + target_folder_name\n",
    "sliced_data_folder = target_folder + '/sliced data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(target_folder):\n",
    "    os.makedirs(target_folder)\n",
    "if not os.path.exists(sliced_data_folder):\n",
    "    os.makedirs(sliced_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for processing all the path files in folder\n",
    "def txt2list(txt_path):\n",
    "    '''\n",
    "    input:\n",
    "        1. txt_path: path to the text file\n",
    "    output:\n",
    "        2. a list containing individual lists, each of which contains:\n",
    "            userid, pathid, itemids in path\n",
    "    '''\n",
    "    results = []\n",
    "    with open(txt_path) as inputfile:\n",
    "        for line in inputfile:\n",
    "            results.append(line.strip().split('|'))\n",
    "            \n",
    "    valid_results = []\n",
    "    for row in results:\n",
    "        if ',' in row[2]:\n",
    "            valid_results.append(row)\n",
    "            \n",
    "    return valid_results\n",
    "\n",
    "def get_paths_from_folder(folder_path):    \n",
    "    return [join(folder_path, f) for f in listdir(folder_path) if isfile(join(folder_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder2arr(folder_path):\n",
    "    # this is a list of all the daily paths (txtfiles)\n",
    "    file_paths = get_paths_from_folder(folder_path)\n",
    "\n",
    "    all_paths = []\n",
    "    for txt_path in file_paths:\n",
    "        if not txt_path.endswith('.DS_Store'):\n",
    "            all_paths = all_paths + txt2list(txt_path)\n",
    "    len(all_paths)    \n",
    "\n",
    "    arr = np.array(all_paths)\n",
    "    arr = arr.astype(object)\n",
    "\n",
    "    # process array so that in each row: element 0 = userid, element 1 = sessid, element 2 = list of itemids\n",
    "    for j in range(len(arr)):\n",
    "        arr[j][0] = int(arr[j][0])\n",
    "        arr[j][1] = int(arr[j][1])\n",
    "        arr[j][2] = list(arr[j][2].split(','))\n",
    "        for i in range(len(arr[j][2])):\n",
    "            arr[j][2][i] = int(arr[j][2][i])\n",
    "\n",
    "    # create a list containing the session lengths\n",
    "    sess_len_list = []\n",
    "    for i in range(arr.shape[0]):\n",
    "        sess_len_list.append(len(arr[i][2]))\n",
    "    sess_len_arr = np.array(sess_len_list)\n",
    "    sess_len_arr = sess_len_arr.reshape([len(sess_len_list),1])\n",
    "\n",
    "    # concatenate the array so that in each row element 3 = session length (number of items presented)\n",
    "    arr = np.concatenate((arr,sess_len_arr),axis = 1)\n",
    "    \n",
    "    # create a new array with desired session length\n",
    "    new_arr = []\n",
    "    for row in arr:\n",
    "        if row[3] >= min_length and row[3] <= max_length:\n",
    "            new_arr.append(row)\n",
    "    new_arr = np.array(new_arr)\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr = folder2arr(train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(arr):\n",
    "    # np array to dataframe for better statistics\n",
    "    df = pd.DataFrame(arr)\n",
    "    df.columns = ['userid', 'sess_id', 'sess_path', 'sess_length']\n",
    "\n",
    "    df = df.sort_values(by = 'sess_length')\n",
    "    df = df.reset_index(drop = 'True')\n",
    "\n",
    "    summary = df.groupby('sess_length').count()\n",
    "    summary = pd.DataFrame(summary.drop(columns=['sess_id','sess_path']))\n",
    "    summary = summary.reset_index()\n",
    "    summary.columns = ['sess_length', 'count']\n",
    "\n",
    "    slice_table = summary\n",
    "    slice_table['start_index'] = slice_table['count']\n",
    "    slice_table['end_index'] = slice_table['count']\n",
    "\n",
    "    # here is the logic for create the start index + end index\n",
    "    for i in range(1, len(slice_table)):\n",
    "        slice_table['end_index'][i] = slice_table['end_index'][i-1] + slice_table['count'][i]\n",
    "\n",
    "    for i in range(1, len(slice_table)):\n",
    "        slice_table['start_index'][i] = slice_table['end_index'][i-1]\n",
    "\n",
    "    slice_table['start_index'][0] = 0\n",
    "\n",
    "    print('The raw data is as followed: ')\n",
    "    print(slice_table)\n",
    "    \n",
    "    return slice_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid_df = pd.read_csv(target_folder + '/userid_map.csv')\n",
    "itemid_df = pd.read_csv(target_folder + '/itemid_map.csv')\n",
    "\n",
    "# below is the dictionary for mapping item/user ids in shopee db into model based id\n",
    "userid_dict = dict([(userid_df['userid'][i], i) for i in range(len(userid_df))])\n",
    "itemid_dict = dict([(itemid_df['itemid'][i], i) for i in range(len(itemid_df))])\n",
    "\n",
    "def sort_and_map(arr):\n",
    "    # sorting\n",
    "    df = pd.DataFrame(arr)\n",
    "    df.columns = ['userid', 'sess_id', 'sess_path', 'sess_length']\n",
    "    df = df.sort_values(by = 'sess_length')\n",
    "    df = df.reset_index(drop = 'True')\n",
    "    arr = df.values\n",
    "    # mapping the shopee userid/itemid into training data index\n",
    "    for i in range(arr.shape[0]):\n",
    "        arr[i][0] = userid_dict[arr[i][0]]\n",
    "        for j in range(len(arr[i][2])):\n",
    "            arr[i][2][j] = itemid_dict[arr[i][2][j]]\n",
    "    return arr\n",
    "\n",
    "train_arr = sort_and_map(train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data is as followed: \n",
      "    sess_length  count  start_index  end_index\n",
      "0             5  73170            0      73170\n",
      "1             6  40952        73170     114122\n",
      "2             7  24961       114122     139083\n",
      "3             8  17025       139083     156108\n",
      "4             9  12074       156108     168182\n",
      "5            10   8117       168182     176299\n",
      "6            11   6698       176299     182997\n",
      "7            12   5280       182997     188277\n",
      "8            13   3699       188277     191976\n",
      "9            14   2964       191976     194940\n",
      "10           15   2660       194940     197600\n",
      "11           16   2039       197600     199639\n",
      "12           17   1460       199639     201099\n",
      "13           18   1263       201099     202362\n",
      "14           19   1172       202362     203534\n",
      "15           20    760       203534     204294\n"
     ]
    }
   ],
   "source": [
    "slice_table = summarize(train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24054, 7, list([56972, 27704, 27703, 30325, 73218]), 5],\n",
       "       [4508, 17, list([268603, 16780, 65518, 138916, 132]), 5],\n",
       "       [13569, 52, list([167594, 100456, 169896, 100620, 57662]), 5],\n",
       "       ...,\n",
       "       [9001, 22,\n",
       "        list([19647, 11802, 25455, 10036, 7129, 85363, 4024, 34407, 85362, 249334, 103496, 103500, 35766, 20413, 9095, 16562, 59215, 46568, 5783, 118062]),\n",
       "        20],\n",
       "       [10637, 27,\n",
       "        list([42952, 45777, 36806, 99393, 90428, 135333, 95011, 80151, 172621, 106914, 151749, 8149, 71764, 31121, 116243, 39153, 110723, 41736, 41735, 41737]),\n",
       "        20],\n",
       "       [18707, 6,\n",
       "        list([179952, 131399, 120910, 23214, 68651, 22129, 44564, 65493, 58320, 65008, 11934, 72181, 53141, 58312, 133875, 17987, 74185, 922, 43862, 175837]),\n",
       "        20]], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14211, 8, list([140867, 174524, 187194, 66103, 258720]), 5],\n",
       "       [24576, 50, list([34795, 274828, 210678, 224771, 127576]), 5],\n",
       "       [3489, 50, list([98659, 145464, 127125, 67534, 98632]), 5],\n",
       "       ...,\n",
       "       [18707, 6,\n",
       "        list([179952, 131399, 120910, 23214, 68651, 22129, 44564, 65493, 58320, 65008, 11934, 72181, 53141, 58312, 133875, 17987, 74185]),\n",
       "        17],\n",
       "       [18707, 6,\n",
       "        list([179952, 131399, 120910, 23214, 68651, 22129, 44564, 65493, 58320, 65008, 11934, 72181, 53141, 58312, 133875, 17987, 74185, 922]),\n",
       "        18],\n",
       "       [18707, 6,\n",
       "        list([179952, 131399, 120910, 23214, 68651, 22129, 44564, 65493, 58320, 65008, 11934, 72181, 53141, 58312, 133875, 17987, 74185, 922, 43862]),\n",
       "        19]], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_arr = []\n",
    "for row in train_arr:\n",
    "    for new_length in range(min_length, row[3]):\n",
    "        new_row = []\n",
    "        new_row.append(row[0])\n",
    "        new_row.append(row[1])\n",
    "        new_row.append(row[2][:new_length])\n",
    "        new_row.append(new_length)\n",
    "        new_data_arr.append(new_row)\n",
    "new_data_arr = np.array(new_data_arr, dtype=object)\n",
    "new_data_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train_arr = np.concatenate((train_arr, new_data_arr))\n",
    "\n",
    "def sort(arr):\n",
    "    # sorting\n",
    "    df = pd.DataFrame(arr)\n",
    "    df.columns = ['userid', 'sess_id', 'sess_path', 'sess_length']\n",
    "    df = df.sort_values(by = 'sess_length')\n",
    "    df = df.reset_index(drop = 'True')\n",
    "    arr = df.values\n",
    "\n",
    "    return arr\n",
    "\n",
    "aug_train_arr = sort(aug_train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data is as followed: \n",
      "    sess_length   count  start_index  end_index\n",
      "0             5  204294            0     204294\n",
      "1             6  131124       204294     335418\n",
      "2             7   90172       335418     425590\n",
      "3             8   65211       425590     490801\n",
      "4             9   48186       490801     538987\n",
      "5            10   36112       538987     575099\n",
      "6            11   27995       575099     603094\n",
      "7            12   21297       603094     624391\n",
      "8            13   16017       624391     640408\n",
      "9            14   12318       640408     652726\n",
      "10           15    9354       652726     662080\n",
      "11           16    6694       662080     668774\n",
      "12           17    4655       668774     673429\n",
      "13           18    3195       673429     676624\n",
      "14           19    1932       676624     678556\n",
      "15           20     760       678556     679316\n"
     ]
    }
   ],
   "source": [
    "aug_slice_table = summarize(aug_train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(arr):\n",
    "    Y_list = []\n",
    "    for i in range(arr.shape[0]):\n",
    "        Y_list.append(arr[i][2][-1])\n",
    "        arr[i][2] = arr[i][2][:-1]\n",
    "    Y_arr = np.array(Y_list)\n",
    "    Y_arr = Y_arr.reshape([len(Y_list),1])\n",
    "\n",
    "    arr = np.concatenate((arr,Y_arr),axis = 1)\n",
    "    return arr\n",
    "\n",
    "aug_train_arr = add_label(aug_train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679315\n"
     ]
    }
   ],
   "source": [
    "buckets_list = [5,6,10,20,50,100]\n",
    "# we deduct all elements in the buckets_list by 1, to fulfill the length of path without Y item\n",
    "for i in range(len(buckets_list)):\n",
    "    buckets_list[i] = buckets_list[i] - 1\n",
    "    \n",
    "def pad2buckets(buckets_list, in_arr):\n",
    "    '''\n",
    "    Input:\n",
    "        1. buckets_list: a list of ints, indicating the step lengths of data we want to generate\n",
    "        2. data_arr: the half-processed data array\n",
    "    Output: \n",
    "        1. pro_arr: the processed array\n",
    "    '''\n",
    "    data_arr = in_arr\n",
    "    # cursor labels the current max length of our training data\n",
    "    cursor = 0\n",
    "    for i in range(data_arr.shape[0]):\n",
    "        # the fourth (index 3) column is the session length of the current user path\n",
    "        if len(data_arr[i][2]) > buckets_list[-1]:\n",
    "            break\n",
    "        # move the cursor to the correct place, \n",
    "        # by right the max_length should be bigger or equal to the length of the current row of data\n",
    "        while len(data_arr[i][2]) > buckets_list[cursor]:\n",
    "            cursor = cursor + 1\n",
    "        \n",
    "        if len(data_arr[i][2]) == buckets_list[cursor]:\n",
    "            data_arr[i][2] = np.array(data_arr[i][2])\n",
    "        if len(data_arr[i][2]) < buckets_list[cursor]:\n",
    "            # create a temp numpy array \n",
    "            #temp_path = np.array([0 for i in range(buckets_list[cursor])])\n",
    "            for j in range(buckets_list[cursor]-len(data_arr[i][2])):\n",
    "                # temp_path[j] = data_arr[i][2][j]\n",
    "                data_arr[i][2].append(0)\n",
    "            # data_arr[i][2] = temp_path\n",
    "            data_arr[i][2] = np.array(data_arr[i][2])\n",
    "            data_arr[i][3] = buckets_list[cursor] + 1\n",
    "    print(i)       \n",
    "    # now i is at the first row we want to abandon\n",
    "    return data_arr[:i]\n",
    "\n",
    "aug_train_arr = pad2buckets(buckets_list, aug_train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the summarization of the augmented training data: \n",
      "   sess_length   count  start_index  end_index\n",
      "0            5  204294            0     204294\n",
      "1            6  131124       204294     335418\n",
      "2           10  239681       335418     575099\n",
      "3           20  104216       575099     679315\n"
     ]
    }
   ],
   "source": [
    "def summarize(pro_arr):\n",
    "    pro_df = pd.DataFrame(pro_arr)\n",
    "    pro_df.columns = ['userid', 'pathid', 'path', 'sess_length', 'Y']\n",
    "    # derive the summary/slice table for the training data\n",
    "    summary = pro_df.groupby('sess_length').count()\n",
    "    summary = pd.DataFrame(summary.drop(columns=['pathid','path','Y']))\n",
    "    summary = summary.reset_index()\n",
    "    summary.columns = ['sess_length', 'count']\n",
    "\n",
    "    slice_table = summary\n",
    "    slice_table['start_index'] = slice_table['count']\n",
    "    slice_table['end_index'] = slice_table['count']\n",
    "    # here is the logic for create the start index + end index\n",
    "    for i in range(1, len(slice_table)):\n",
    "        slice_table['end_index'][i] = slice_table['end_index'][i-1] + slice_table['count'][i]\n",
    "\n",
    "    for i in range(1, len(slice_table)):\n",
    "        slice_table['start_index'][i] = slice_table['end_index'][i-1]\n",
    "\n",
    "    slice_table['start_index'][0] = 0\n",
    "    return slice_table\n",
    "\n",
    "print(\"Here is the summarization of the augmented training data: \")\n",
    "print(summarize(aug_train_arr))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(sliced_data_folder +'/X_train.npy', aug_train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
